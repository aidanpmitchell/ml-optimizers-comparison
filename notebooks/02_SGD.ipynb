{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18316924",
   "metadata": {},
   "source": [
    "# Optimizer #2: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645497ab",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af5843",
   "metadata": {},
   "source": [
    "A few problems arise from vanilla gradient descent and its applications:\n",
    "1. **Slow with large datasets**: For each single training step, gradient descent uses **ALL** of the training data\n",
    "    - For example, if we had a more complicated model such as a logistic regression model that used 10,000 parameters to predict something, and we had data from 1,000,000 samples, we would have to calculate 10,000,000,000 terms for each step. Doing this over, say 1,000 steps, would result in the need to calculate at least 1,000,000,000,000 terms!\n",
    "2. **Gets stuck in flat regions or local minima**: Because it always moves in the direction of the exact full gradient, it may settle into shallow minima, saddle points, or flat plateaus.\n",
    "3. **Memory inefficiency**: It must hold the full dataset in memory (or at least compute the full batch gradient), which isn't scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733e0c6",
   "metadata": {},
   "source": [
    "Unlike standard gradient descent — which computes the gradient using the entire dataset — **Stochastic Gradient Descent** updates the model using only a single data point (or a small batch) at a time. This fixes the problems that arise with regular gradient descent, because it provides:\n",
    "- **Faster updates**: It updates the model more frequently as it uses single points or small batches to compute the gradient, which leads to much faster iterations, especially on larger datasets.\n",
    "- **Better exploration**: The inherent noise in mini batch iterations gradients adds randomness, which helps to escape local minima and saddle points more efficiently.\n",
    "- **Memory efficiency**: Only a small batch is needed at a time, which lowers the overall memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b238448",
   "metadata": {},
   "source": [
    "Because of this key change, the update rule becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5cff6",
   "metadata": {},
   "source": [
    "## Update Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30030fe",
   "metadata": {},
   "source": [
    "The core idea of SGD is to update your model’s parameters $\\theta$ using the gradient of the loss function computed on just one sample (or a small batch) at a time.\n",
    "\n",
    "General form:\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta; x_i, y_i)\n",
    "$$\n",
    "Where:\n",
    "- $\\theta$: the vector of model parameters (weights and biases)\n",
    "- $\\eta$: the learning rate\n",
    "- $\\nabla_\\theta \\mathcal{L}(\\theta; x_i, y_i)$: gradient of the loss function with respect to the parameters, evaluated at a single training example ($x_i, y_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457b036",
   "metadata": {},
   "source": [
    "Intuitively, each training sample tells you a slightly noisy estimate of how the parameters should change to reduce the loss. By updating the parameters immediately using just that estimate, the model starts adapting right away — instead of waiting for the average over the entire dataset (as in batch gradient descent). This makes SGD much more efficient on larger datasets, and faster to respond to local patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0911b",
   "metadata": {},
   "source": [
    "### In Practice (Mini-Batch SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d0f45",
   "metadata": {},
   "source": [
    "Instead of just using one data point, it is common to use mini-batches of the data, of size B. This leads to:\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\cdot \\frac{1}{B} \\sum_{j=1}^{B} \\nabla_\\theta \\mathcal{L}(\\theta; x_i, y_i)\n",
    "$$\n",
    "This balances:\n",
    "- **Efficiency** (processing multiple samples at once)\n",
    "- **Stability** (less noise than single point sample)\n",
    "- **Exploration** (more randomness than full batch updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ee672",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
