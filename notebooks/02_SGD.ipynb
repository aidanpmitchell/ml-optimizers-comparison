{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18316924",
   "metadata": {},
   "source": [
    "# Optimizer #2: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645497ab",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af5843",
   "metadata": {},
   "source": [
    "A few problems arise from vanilla gradient descent and its applications:\n",
    "1. **Slow with large datasets**: For each single training step, gradient descent uses **ALL** of the training data\n",
    "    - For example, if we had a more complicated model such as a logistic regression model that used 10,000 parameters to predict something, and we had data from 1,000,000 samples, we would have to calculate 10,000,000,000 terms for each step. Doing this over, say 1,000 steps, would result in the need to calculate at least 1,000,000,000,000 terms!\n",
    "2. **Gets stuck in flat regions or local minima**: Because it always moves in the direction of the exact full gradient, it may settle into shallow minima, saddle points, or flat plateaus.\n",
    "3. **Memory inefficiency**: It must hold the full dataset in memory (or at least compute the full batch gradient), which isn't scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733e0c6",
   "metadata": {},
   "source": [
    "Unlike standard gradient descent — which computes the gradient using the entire dataset — **Stochastic Gradient Descent** updates the model using only a single data point (or a small batch) at a time. This fixes the problems that arise with regular gradient descent, because it provides:\n",
    "- **Faster updates**: It updates the model more frequently as it uses single points or small batches to compute the gradient, which leads to much faster iterations, especially on larger datasets.\n",
    "- **Better exploration**: The inherent noise in mini batch iterations gradients adds randomness, which helps to escape local minima and saddle points more efficiently.\n",
    "- **Memory efficiency**: Only a small batch is needed at a time, which lowers the overall memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b238448",
   "metadata": {},
   "source": [
    "Because of this key change, the update rule becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5cff6",
   "metadata": {},
   "source": [
    "## Update Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30030fe",
   "metadata": {},
   "source": [
    "The core idea of SGD is to update your model’s parameters $\\theta$ using the gradient of the loss function computed on just one sample (or a small batch) at a time.\n",
    "\n",
    "General form:\n",
    "$$\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(\\theta; x_i, y_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457b036",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
